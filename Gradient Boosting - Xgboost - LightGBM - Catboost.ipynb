{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNeJ50PETfqpzl5no28Du4q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Random Forest"],"metadata":{"id":"b-wcEL6_2kKa"}},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","\n","\n","\n","def gini_impurity(y):\n","    \"\"\"\n","    Calculate the Gini Impurity for an array of class labels.\n","    \"\"\"\n","    if len(y) == 0:\n","        return 0\n","    counts = np.bincount(y)\n","    probabilities = counts / len(y)\n","    return 1 - np.sum(probabilities ** 2)\n","\n","def best_split(X, y, feature_indices):\n","    \"\"\"\n","    Find the best split for the data in X, y using only the features in feature_indices.\n","    Returns:\n","      - best_feature: index of the best feature to split on\n","      - best_threshold: threshold value for the split\n","      - best_gain: the impurity gain from the best split\n","      - best_left_idx, best_right_idx: indices of the data points going to left and right splits\n","    \"\"\"\n","    best_feature = None\n","    best_threshold = None\n","    best_gain = -np.inf\n","    best_left_idx = None\n","    best_right_idx = None\n","    parent_impurity = gini_impurity(y)\n","    n_samples = X.shape[0]\n","\n","    if n_samples < 2:\n","        return None, None, None, None, None\n","\n","    for feature in feature_indices:\n","        # Get all unique values for this feature as potential thresholds\n","        thresholds = np.unique(X[:, feature])\n","        for threshold in thresholds:\n","            # Split the dataset into left/right groups\n","            left_idx = np.where(X[:, feature] <= threshold)[0]\n","            right_idx = np.where(X[:, feature] > threshold)[0]\n","            if len(left_idx) == 0 or len(right_idx) == 0:\n","                continue  # skip useless splits\n","\n","            # Calculate impurity for children nodes\n","            left_impurity = gini_impurity(y[left_idx])\n","            right_impurity = gini_impurity(y[right_idx])\n","            # Compute weighted impurity of the split\n","            weighted_impurity = (len(left_idx) / n_samples) * left_impurity + \\\n","                                (len(right_idx) / n_samples) * right_impurity\n","            gain = parent_impurity - weighted_impurity\n","\n","            if gain > best_gain:\n","                best_gain = gain\n","                best_feature = feature\n","                best_threshold = threshold\n","                best_left_idx = left_idx\n","                best_right_idx = right_idx\n","\n","    return best_feature, best_threshold, best_gain, best_left_idx, best_right_idx\n","\n","class Node:\n","    \"\"\"\n","    A node in the decision tree.\n","    \"\"\"\n","    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n","        # For internal nodes:\n","        self.feature = feature      # index of feature to split on\n","        self.threshold = threshold  # threshold value for the split\n","        self.left = left            # left child node (X[feature] <= threshold)\n","        self.right = right          # right child node (X[feature] > threshold)\n","        # For leaf nodes:\n","        self.value = value          # class label (for classification)\n","\n","# -------------------------------\n","# Decision Tree Implementation\n","# -------------------------------\n","\n","def build_tree(X, y, max_depth=None, min_samples_split=2, n_features=None, depth=0):\n","    \"\"\"\n","    Recursively build a decision tree.\n","    - X: features (numpy array of shape [n_samples, n_features])\n","    - y: labels (numpy array of shape [n_samples])\n","    - max_depth: maximum depth of the tree (None means unlimited)\n","    - min_samples_split: minimum number of samples required to split\n","    - n_features: number of random features to consider at each split (if None, use all)\n","    - depth: current depth of the tree\n","    \"\"\"\n","    n_samples, n_total_features = X.shape\n","    num_labels = len(np.unique(y))\n","\n","    # Stopping conditions: max depth reached, pure node, or too few samples to split.\n","    if (max_depth is not None and depth >= max_depth) or num_labels == 1 or n_samples < min_samples_split:\n","        # Create a leaf node with the most common label\n","        leaf_value = Counter(y).most_common(1)[0][0]\n","        return Node(value=leaf_value)\n","\n","    # Randomly select subset of features if specified\n","    if n_features is None:\n","        feature_indices = list(range(n_total_features))\n","    else:\n","        feature_indices = np.random.choice(n_total_features, n_features, replace=False)\n","\n","    # Find the best split among the chosen features\n","    best = best_split(X, y, feature_indices)\n","    feature, threshold, gain, left_idx, right_idx = best\n","\n","    # If no valid split was found, create a leaf node\n","    if feature is None or gain == 0:\n","        leaf_value = Counter(y).most_common(1)[0][0]\n","        return Node(value=leaf_value)\n","\n","    # Recursively build the left and right subtrees\n","    left_subtree = build_tree(X[left_idx, :], y[left_idx],\n","                              max_depth=max_depth,\n","                              min_samples_split=min_samples_split,\n","                              n_features=n_features,\n","                              depth=depth+1)\n","    right_subtree = build_tree(X[right_idx, :], y[right_idx],\n","                               max_depth=max_depth,\n","                               min_samples_split=min_samples_split,\n","                               n_features=n_features,\n","                               depth=depth+1)\n","\n","    return Node(feature=feature, threshold=threshold, left=left_subtree, right=right_subtree)\n","\n","def predict_tree(x, tree):\n","    \"\"\"\n","    Predict the class label for a single sample x using the decision tree.\n","    \"\"\"\n","    # If the node is a leaf, return its value\n","    if tree.value is not None:\n","        return tree.value\n","    # Otherwise, follow the appropriate branch\n","    if x[tree.feature] <= tree.threshold:\n","        return predict_tree(x, tree.left)\n","    else:\n","        return predict_tree(x, tree.right)\n","\n","# -------------------------------\n","# Random Forest Implementation\n","# -------------------------------\n","\n","class RandomForest:\n","    def __init__(self, n_trees=10, max_depth=None, min_samples_split=2, n_features=None, bootstrap=True):\n","        \"\"\"\n","        Parameters:\n","          - n_trees: Number of trees in the forest.\n","          - max_depth: Maximum depth for each tree.\n","          - min_samples_split: Minimum samples required to split a node.\n","          - n_features: Number of features to consider when looking for the best split.\n","                        (If None, then use all features.)\n","          - bootstrap: Whether to use bootstrap samples when building trees.\n","        \"\"\"\n","        self.n_trees = n_trees\n","        self.max_depth = max_depth\n","        self.min_samples_split = min_samples_split\n","        self.n_features = n_features\n","        self.bootstrap = bootstrap\n","        self.trees = []\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Build the forest of trees from the training set.\n","        \"\"\"\n","        self.trees = []\n","        n_samples = X.shape[0]\n","        for _ in range(self.n_trees):\n","            # Create a bootstrap sample\n","            if self.bootstrap:\n","                indices = np.random.choice(n_samples, n_samples, replace=True)\n","                X_sample = X[indices]\n","                y_sample = y[indices]\n","            else:\n","                X_sample = X\n","                y_sample = y\n","            # Build a decision tree on the sample\n","            tree = build_tree(X_sample, y_sample,\n","                              max_depth=self.max_depth,\n","                              min_samples_split=self.min_samples_split,\n","                              n_features=self.n_features)\n","            self.trees.append(tree)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict class labels for samples in X.\n","        Returns an array of predictions using majority voting.\n","        \"\"\"\n","        # Collect predictions from each tree\n","        tree_preds = np.array([[predict_tree(x, tree) for tree in self.trees] for x in X])\n","        # Majority vote: for each sample, choose the most common class prediction\n","        y_pred = [Counter(tree_pred).most_common(1)[0][0] for tree_pred in tree_preds]\n","        return np.array(y_pred)\n"],"metadata":{"id":"wwpCKYte2mZZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Gradient Boosting\n","\n","##  Problem Setup\n","\n","Given a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$ (for regression), we aim to minimize:\n","\n","$$\n","\\mathcal{L}(F) = \\sum_{i=1}^N L(y_i, F(x_i))\n","$$\n","\n","where $F(x)$ is an additive model:\n","\n","$$\n","F(x) = \\sum_{m=0}^{M} \\gamma_m h_m(x).\n","$$\n","\n","## Functional Gradient Descent\n","\n","Gradient descent in parameter space:\n","\n","$$\n","\\theta_m = \\theta_{m-1} - \\eta \\nabla_\\theta \\mathcal{L}(\\theta_{m-1}).\n","$$\n","\n","Gradient descent in function space:\n","\n","$$\n","F_m(x) = F_{m-1}(x) - \\eta \\nabla_F \\mathcal{L}(F_{m-1}).\n","$$\n","\n","At each step, we approximate the negative gradient:\n","\n","$$\n","r_{im} = - \\left. \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right|_{F = F_{m-1}}.\n","$$\n","\n","and fit a weak learner:\n","\n","$$\n","h_m = \\arg \\min_{h \\in \\mathcal{H}} \\sum_{i=1}^N (r_{im} - h(x_i))^2.\n","$$\n","\n","##  Algorithm\n","\n","1. **Initialize:**  \n","   $F_0(x) = \\arg \\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma)$.\n","2. **For** $m = 1, \\dots, M$:\n","   - Compute pseudo-residuals:\n","     $$\n","     r_{im} = - \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}.\n","     $$\n","   - Fit weak learner $h_m$ to $\\{(x_i, r_{im})\\}$.\n","   - Find optimal weight:\n","     $$\n","     \\gamma_m = \\arg \\min_{\\gamma} \\sum_{i=1}^N L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)).\n","     $$\n","   - Update:\n","     $$\n","     F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x).\n","     $$\n","\n","## Example: Squared Error Loss\n","\n","For $L(y, F) = \\frac{1}{2} (y - F)^2$, we get:\n","\n","$$\n","r_{im} = y_i - F_{m-1}(x_i).\n","$$\n","\n","Thus, gradient boosting reduces to stage-wise residual fitting."],"metadata":{"id":"cRSbGJ0Cni97"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcSHL-XUnc-5"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n","from sklearn.datasets import make_regression, make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, accuracy_score\n","\n","# ---- Regression Example ----\n","X_reg, y_reg = make_regression(n_samples=500, n_features=1, noise=15, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n","\n","gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n","gbr.fit(X_train, y_train)\n","\n","y_pred = gbr.predict(X_test)\n","print(f\"Regression MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n","\n","# Plot regression results\n","plt.scatter(X_test, y_test, color=\"blue\", label=\"True\")\n","plt.scatter(X_test, y_pred, color=\"red\", alpha=0.6, label=\"Predicted\")\n","plt.title(\"Gradient Boosting Regression\")\n","plt.legend()\n","plt.show()\n","\n","# ---- Classification Example ----\n","X_cls, y_cls = make_classification(n_samples=500, n_features=5, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n","\n","gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n","gbc.fit(X_train, y_train)\n","\n","y_pred = gbc.predict(X_test)\n","print(f\"Classification Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"]},{"cell_type":"markdown","source":["# XGBoost\n","\n","\n","## Objective Function\n","\n","Given data $\\{(x_i, y_i)\\}_{i=1}^n$ and loss $\\ell(y_i, \\hat{y}_i)$, XGBoost minimizes:\n","$$\n","\\mathcal{L}(F) = \\sum_{i=1}^n \\ell(y_i, \\hat{y}_i) + \\sum_{t=1}^T \\Omega(f_t),\n","$$\n","where $\\Omega(f_t)$ penalizes tree complexity:\n","$$\n","\\Omega(f_t) = \\gamma T_{\\text{leaf}} + \\frac{1}{2} \\lambda \\sum_{j=1}^{T_{\\text{leaf}}} w_j^2.\n","$$\n","\n","## Second-Order Approximation\n","\n","For a new tree $f_t$, predictions update as:\n","$$\n","\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i).\n","$$\n","Using Taylor expansion:\n","$$\n","\\ell(y_i, \\hat{y}_i^{(t)}) \\approx \\ell(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2,\n","$$\n","where\n","$$\n","g_i = \\frac{\\partial \\ell}{\\partial \\hat{y}_i}, \\quad h_i = \\frac{\\partial^2 \\ell}{\\partial \\hat{y}_i^2}.\n","$$\n","Thus, the objective reduces to:\n","$$\n","\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n (g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2) + \\Omega(f_t).\n","$$\n","\n","##  Tree Structure and Optimal Weights\n","\n","Each tree assigns samples to leaves:\n","$$\n","f_t(x) = w_{q(x)},\n","$$\n","where $q(x)$ maps $x$ to a leaf index. Let:\n","$$\n","G_j = \\sum_{i \\in I_j} g_i, \\quad H_j = \\sum_{i \\in I_j} h_i.\n","$$\n","Then the objective simplifies to:\n","$$\n","\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T_{\\text{leaf}}} \\left(G_j w_j + \\frac{1}{2} H_j w_j^2 \\right) + \\gamma T_{\\text{leaf}} + \\frac{1}{2}\\lambda \\sum_{j=1}^{T_{\\text{leaf}}} w_j^2.\n","$$\n","\n","###  Computing Optimal Leaf Weights\n","\n","To find the best leaf weights $\\{ w_j \\}$, we minimize the objective with respect to each $w_j$:\n","$$\n","\\frac{\\partial}{\\partial w_j} \\left(G_j w_j + \\frac{1}{2} H_j w_j^2 + \\frac{1}{2} \\lambda w_j^2 \\right) = G_j + (H_j + \\lambda) w_j = 0.\n","$$\n","Solving for $w_j^*$:\n","$$\n","w_j^* = -\\frac{G_j}{H_j + \\lambda}.\n","$$\n","\n","###  Gain from Adding a Tree\n","\n","Plugging $w_j^*$ into the objective, the reduction in loss (gain) from adding a tree is:\n","$$\n","\\text{Gain} = -\\frac{1}{2} \\sum_{j=1}^{T_{\\text{leaf}}} \\frac{G_j^2}{H_j + \\lambda} + \\gamma T_{\\text{leaf}}.\n","$$\n","A larger gain means a better split.\n","\n","##  Mean Squared Error (MSE) Loss\n","\n","For squared loss:\n","$$\n","\\ell(y_i, \\hat{y}_i) = \\frac{1}{2} (y_i - \\hat{y}_i)^2.\n","$$\n","Gradients:\n","$$\n","g_i = \\frac{\\partial}{\\partial \\hat{y}_i} \\left[\\frac{1}{2} (y_i - \\hat{y}_i)^2\\right] = \\hat{y}_i - y_i,\n","$$\n","$$\n","h_i = \\frac{\\partial^2}{\\partial \\hat{y}_i^2} \\left[\\frac{1}{2} (y_i - \\hat{y}_i)^2\\right] = 1.\n","$$\n","Then:\n","$$\n","G_j = \\sum_{i \\in I_j} (\\hat{y}_i - y_i), \\quad H_j = |I_j|.\n","$$\n","Optimal leaf weight:\n","$$\n","w_j^* = -\\frac{\\sum_{i \\in I_j} (\\hat{y}_i - y_i)}{|I_j| + \\lambda}.\n","$$\n","\n","## Split Finding\n","\n","For a potential split into left ($I_L$) and right ($I_R$):\n","$$\n","\\text{Gain} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G^2}{H + \\lambda} \\right) - \\gamma.\n","$$\n","XGBoost picks the split maximizing this gain.\n"],"metadata":{"id":"LkEAVNVFpZbm"}},{"cell_type":"code","source":["len(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3r1juhmJ-eEn","executionInfo":{"status":"ok","timestamp":1738941269419,"user_tz":300,"elapsed":20,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"ed22d445-84dc-477f-ac41-4355438a2b88"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["70000"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# ------------------------------\n","# Load and preprocess MNIST data\n","# ------------------------------\n","\n","# Fetch the MNIST dataset from OpenML\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","X, y = mnist.data, mnist.target\n","\n","# Convert target labels to integers (they may come as strings)\n","y = y.astype(np.int32)\n","\n","# Normalize pixel values to the range [0, 1]\n","X = X / 255.0\n","\n","# Split the data into training and testing sets.\n","# Here we use 60,000 samples for training and 10,000 for testing.\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=10000, random_state=42, stratify=y\n",")\n","\n","# ------------------------------\n","# Train using XGBClassifier\n","# ------------------------------\n","\n","# Initialize the XGBClassifier.\n","# - 'objective' is set to 'multi:softprob' to get class probabilities.\n","# - 'num_class' is set to 10 (digits 0 through 9).\n","# - 'use_label_encoder' is disabled to avoid warnings (XGBoost 1.3+).\n","# - 'eval_metric' is set to 'mlogloss' (multiclass log-loss).\n","clf = xgb.XGBClassifier(\n","    objective='multi:softprob',\n","    num_class=10,\n","    learning_rate=0.1,\n","    max_depth=6,\n","    n_estimators=100,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42,\n","    use_label_encoder=False,  # Avoids deprecation warnings in newer versions.\n","    eval_metric='mlogloss'\n",")\n","\n","# Set up an evaluation set for early stopping.\n","eval_set = [(X_test, y_test)]\n","\n","# Train the model with early stopping if there is no improvement after 10 rounds.\n","clf.fit(X_train, y_train,  eval_set=eval_set, verbose=True)\n","\n","# Make predictions on the test set.\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate and print the accuracy.\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"XGBClassifier Accuracy on MNIST: {:.2f}%\".format(accuracy * 100))\n"],"metadata":{"id":"9faxnH4Up3Z1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738942007959,"user_tz":300,"elapsed":694682,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"f7b9f5d7-e5d7-46d0-f788-5f3495d9465f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [15:15:28] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-mlogloss:1.96966\n","[1]\tvalidation_0-mlogloss:1.73759\n","[2]\tvalidation_0-mlogloss:1.55649\n","[3]\tvalidation_0-mlogloss:1.41114\n","[4]\tvalidation_0-mlogloss:1.28926\n","[5]\tvalidation_0-mlogloss:1.18460\n","[6]\tvalidation_0-mlogloss:1.09344\n","[7]\tvalidation_0-mlogloss:1.01279\n","[8]\tvalidation_0-mlogloss:0.94136\n","[9]\tvalidation_0-mlogloss:0.87810\n","[10]\tvalidation_0-mlogloss:0.82030\n","[11]\tvalidation_0-mlogloss:0.76680\n","[12]\tvalidation_0-mlogloss:0.72017\n","[13]\tvalidation_0-mlogloss:0.67736\n","[14]\tvalidation_0-mlogloss:0.63858\n","[15]\tvalidation_0-mlogloss:0.60291\n","[16]\tvalidation_0-mlogloss:0.57040\n","[17]\tvalidation_0-mlogloss:0.54046\n","[18]\tvalidation_0-mlogloss:0.51265\n","[19]\tvalidation_0-mlogloss:0.48705\n","[20]\tvalidation_0-mlogloss:0.46426\n","[21]\tvalidation_0-mlogloss:0.44253\n","[22]\tvalidation_0-mlogloss:0.42272\n","[23]\tvalidation_0-mlogloss:0.40413\n","[24]\tvalidation_0-mlogloss:0.38703\n","[25]\tvalidation_0-mlogloss:0.37096\n","[26]\tvalidation_0-mlogloss:0.35607\n","[27]\tvalidation_0-mlogloss:0.34261\n","[28]\tvalidation_0-mlogloss:0.33019\n","[29]\tvalidation_0-mlogloss:0.31796\n","[30]\tvalidation_0-mlogloss:0.30676\n","[31]\tvalidation_0-mlogloss:0.29631\n","[32]\tvalidation_0-mlogloss:0.28671\n","[33]\tvalidation_0-mlogloss:0.27787\n","[34]\tvalidation_0-mlogloss:0.26914\n","[35]\tvalidation_0-mlogloss:0.26066\n","[36]\tvalidation_0-mlogloss:0.25287\n","[37]\tvalidation_0-mlogloss:0.24545\n","[38]\tvalidation_0-mlogloss:0.23858\n","[39]\tvalidation_0-mlogloss:0.23198\n","[40]\tvalidation_0-mlogloss:0.22608\n","[41]\tvalidation_0-mlogloss:0.22036\n","[42]\tvalidation_0-mlogloss:0.21517\n","[43]\tvalidation_0-mlogloss:0.21041\n","[44]\tvalidation_0-mlogloss:0.20589\n","[45]\tvalidation_0-mlogloss:0.20150\n","[46]\tvalidation_0-mlogloss:0.19730\n","[47]\tvalidation_0-mlogloss:0.19328\n","[48]\tvalidation_0-mlogloss:0.18934\n","[49]\tvalidation_0-mlogloss:0.18568\n","[50]\tvalidation_0-mlogloss:0.18209\n","[51]\tvalidation_0-mlogloss:0.17870\n","[52]\tvalidation_0-mlogloss:0.17555\n","[53]\tvalidation_0-mlogloss:0.17251\n","[54]\tvalidation_0-mlogloss:0.16932\n","[55]\tvalidation_0-mlogloss:0.16663\n","[56]\tvalidation_0-mlogloss:0.16352\n","[57]\tvalidation_0-mlogloss:0.16104\n","[58]\tvalidation_0-mlogloss:0.15863\n","[59]\tvalidation_0-mlogloss:0.15618\n","[60]\tvalidation_0-mlogloss:0.15394\n","[61]\tvalidation_0-mlogloss:0.15174\n","[62]\tvalidation_0-mlogloss:0.14962\n","[63]\tvalidation_0-mlogloss:0.14773\n","[64]\tvalidation_0-mlogloss:0.14578\n","[65]\tvalidation_0-mlogloss:0.14386\n","[66]\tvalidation_0-mlogloss:0.14196\n","[67]\tvalidation_0-mlogloss:0.14020\n","[68]\tvalidation_0-mlogloss:0.13837\n","[69]\tvalidation_0-mlogloss:0.13652\n","[70]\tvalidation_0-mlogloss:0.13503\n","[71]\tvalidation_0-mlogloss:0.13321\n","[72]\tvalidation_0-mlogloss:0.13169\n","[73]\tvalidation_0-mlogloss:0.13010\n","[74]\tvalidation_0-mlogloss:0.12880\n","[75]\tvalidation_0-mlogloss:0.12738\n","[76]\tvalidation_0-mlogloss:0.12617\n","[77]\tvalidation_0-mlogloss:0.12482\n","[78]\tvalidation_0-mlogloss:0.12349\n","[79]\tvalidation_0-mlogloss:0.12239\n","[80]\tvalidation_0-mlogloss:0.12140\n","[81]\tvalidation_0-mlogloss:0.12017\n","[82]\tvalidation_0-mlogloss:0.11898\n","[83]\tvalidation_0-mlogloss:0.11770\n","[84]\tvalidation_0-mlogloss:0.11672\n","[85]\tvalidation_0-mlogloss:0.11558\n","[86]\tvalidation_0-mlogloss:0.11436\n","[87]\tvalidation_0-mlogloss:0.11348\n","[88]\tvalidation_0-mlogloss:0.11253\n","[89]\tvalidation_0-mlogloss:0.11147\n","[90]\tvalidation_0-mlogloss:0.11057\n","[91]\tvalidation_0-mlogloss:0.10975\n","[92]\tvalidation_0-mlogloss:0.10893\n","[93]\tvalidation_0-mlogloss:0.10798\n","[94]\tvalidation_0-mlogloss:0.10729\n","[95]\tvalidation_0-mlogloss:0.10658\n","[96]\tvalidation_0-mlogloss:0.10584\n","[97]\tvalidation_0-mlogloss:0.10513\n","[98]\tvalidation_0-mlogloss:0.10434\n","[99]\tvalidation_0-mlogloss:0.10363\n","XGBClassifier Accuracy on MNIST: 97.03%\n"]}]},{"cell_type":"code","source":["# Get the booster from the trained model\n","booster = clf.get_booster()\n","\n","# Get feature importance scores by 'gain'\n","importance_gain = booster.get_score(importance_type='gain')\n","print(\"\\nFeature Importances (Booster by Gain):\")\n","for feature, score in importance_gain.items():\n","    print(f\"{feature}: {score:.4f}\")\n","\n","# Optionally, get feature importance by 'weight'\n","importance_weight = booster.get_score(importance_type='weight')\n","print(\"\\nFeature Importances (Booster by Weight):\")\n","for feature, score in importance_weight.items():\n","    print(f\"{feature}: {score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T035Hr6T9WLo","executionInfo":{"status":"ok","timestamp":1738942016604,"user_tz":300,"elapsed":150,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"d0bfd56c-7873-48ad-87d9-7affe29d29c5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Feature Importances (Booster by Gain):\n","f43: 0.7801\n","f44: 0.8413\n","f45: 0.7382\n","f66: 209.5063\n","f67: 1.6797\n","f68: 3.6039\n","f69: 127.8272\n","f70: 15.3886\n","f71: 1.8951\n","f72: 0.7592\n","f73: 0.9414\n","f74: 6.9372\n","f75: 5.1857\n","f76: 8.0437\n","f91: 6.5453\n","f92: 24.6509\n","f93: 38.9295\n","f94: 97.2169\n","f95: 39.5877\n","f96: 102.5253\n","f97: 60.2205\n","f98: 43.0340\n","f99: 38.8932\n","f100: 196.0658\n","f101: 263.4692\n","f102: 3.2953\n","f103: 87.7536\n","f104: 67.5673\n","f105: 4.0847\n","f106: 32.6529\n","f107: 16.2954\n","f108: 22.9668\n","f117: 2.7063\n","f118: 0.6703\n","f119: 2.0409\n","f120: 3.2424\n","f121: 3.7695\n","f122: 5.1171\n","f123: 7.2589\n","f124: 44.1661\n","f125: 30.7760\n","f126: 44.9572\n","f127: 11.6935\n","f128: 10.1957\n","f129: 4.2349\n","f130: 4.4879\n","f131: 5.2684\n","f132: 4.9363\n","f133: 3.6200\n","f134: 5.3379\n","f135: 7.3938\n","f136: 1.1070\n","f137: 1.0595\n","f143: 0.8043\n","f144: 1.8642\n","f145: 1.1745\n","f146: 2.7340\n","f147: 4.4092\n","f148: 5.2990\n","f149: 18.7248\n","f150: 14.8288\n","f151: 7.3491\n","f152: 10.0654\n","f153: 18.9435\n","f154: 24.6561\n","f155: 139.4860\n","f156: 105.4243\n","f157: 12.6328\n","f158: 18.8394\n","f159: 8.1405\n","f160: 5.9956\n","f161: 3.2209\n","f162: 4.0890\n","f163: 33.5603\n","f164: 11.6283\n","f165: 2.4947\n","f166: 2.6000\n","f171: 0.8289\n","f172: 1.4380\n","f173: 3.6686\n","f174: 12.1159\n","f175: 7.6239\n","f176: 13.0155\n","f177: 69.4291\n","f178: 32.1027\n","f179: 18.8708\n","f180: 9.5249\n","f181: 7.5044\n","f182: 11.5081\n","f183: 14.8357\n","f184: 9.3104\n","f185: 10.8124\n","f186: 13.1367\n","f187: 11.6636\n","f188: 9.7392\n","f189: 4.0424\n","f190: 9.7302\n","f191: 13.0339\n","f192: 3.9684\n","f193: 1.3574\n","f199: 1.7264\n","f200: 2.5758\n","f201: 5.0452\n","f202: 3.5004\n","f203: 16.9259\n","f204: 8.5769\n","f205: 24.9032\n","f206: 16.0151\n","f207: 8.4895\n","f208: 13.9479\n","f209: 16.3953\n","f210: 43.0528\n","f211: 171.8026\n","f212: 21.1104\n","f213: 16.4624\n","f214: 10.2039\n","f215: 8.4581\n","f216: 13.1945\n","f217: 40.1765\n","f218: 16.5786\n","f219: 42.3443\n","f220: 206.9300\n","f221: 1.1525\n","f222: 1.5223\n","f227: 1.0875\n","f228: 3.5503\n","f229: 2.1549\n","f230: 9.5764\n","f231: 44.7581\n","f232: 7.0591\n","f233: 13.8539\n","f234: 36.9689\n","f235: 49.2917\n","f236: 7.0081\n","f237: 7.4728\n","f238: 6.9131\n","f239: 9.6897\n","f240: 5.6844\n","f241: 7.5824\n","f242: 60.3298\n","f243: 45.8200\n","f244: 12.7743\n","f245: 29.5625\n","f246: 7.0108\n","f247: 5.0845\n","f248: 5.0572\n","f249: 2.4821\n","f250: 3.5395\n","f255: 1.0484\n","f256: 3.8248\n","f257: 2.2010\n","f258: 2.3474\n","f259: 3.2809\n","f260: 6.0545\n","f261: 9.0260\n","f262: 28.4898\n","f263: 6.2842\n","f264: 7.3527\n","f265: 12.2762\n","f266: 13.9417\n","f267: 28.2846\n","f268: 16.3411\n","f269: 12.4784\n","f270: 62.5612\n","f271: 15.0961\n","f272: 10.9485\n","f273: 17.2267\n","f274: 10.9534\n","f275: 14.7677\n","f276: 45.9075\n","f277: 47.0570\n","f278: 1.0859\n","f282: 94.0985\n","f283: 78.2225\n","f284: 4.7583\n","f285: 2.7677\n","f286: 2.7467\n","f287: 5.0120\n","f288: 8.6913\n","f289: 27.1226\n","f290: 62.8303\n","f291: 13.5630\n","f292: 6.5094\n","f293: 9.0508\n","f294: 26.3945\n","f295: 15.9136\n","f296: 18.1141\n","f297: 9.0323\n","f298: 27.3485\n","f299: 17.7738\n","f300: 21.8334\n","f301: 23.6321\n","f302: 6.5585\n","f303: 6.6059\n","f304: 10.4802\n","f305: 6.6220\n","f306: 0.9552\n","f311: 4.7298\n","f312: 8.6353\n","f313: 2.2160\n","f314: 7.7032\n","f315: 19.8846\n","f316: 16.3442\n","f317: 21.3251\n","f318: 42.9395\n","f319: 36.7700\n","f320: 14.9993\n","f321: 16.3582\n","f322: 16.1664\n","f323: 11.5904\n","f324: 9.1749\n","f325: 8.4935\n","f326: 21.4358\n","f327: 20.6090\n","f328: 51.8839\n","f329: 38.5926\n","f330: 16.5608\n","f331: 16.5283\n","f332: 2.8322\n","f333: 5.8902\n","f334: 4.5583\n","f339: 6.8049\n","f340: 2.8053\n","f341: 4.0804\n","f342: 9.7204\n","f343: 57.4826\n","f344: 43.1068\n","f345: 22.7444\n","f346: 73.1122\n","f347: 28.4354\n","f348: 12.1237\n","f349: 29.2773\n","f350: 84.0824\n","f351: 24.0373\n","f352: 7.7523\n","f353: 12.9094\n","f354: 13.6164\n","f355: 10.0282\n","f356: 20.4187\n","f357: 12.5372\n","f358: 107.3103\n","f359: 15.1830\n","f360: 3.1243\n","f361: 2.4895\n","f362: 0.8541\n","f367: 3.6036\n","f368: 4.3469\n","f369: 6.3998\n","f370: 23.6202\n","f371: 21.2964\n","f372: 8.8781\n","f373: 16.9174\n","f374: 16.6553\n","f375: 27.4193\n","f376: 20.2048\n","f377: 17.7988\n","f378: 41.7666\n","f379: 14.3428\n","f380: 30.3720\n","f381: 12.4792\n","f382: 8.0094\n","f383: 7.2230\n","f384: 7.0446\n","f385: 4.0915\n","f386: 27.3201\n","f387: 8.0133\n","f388: 4.5510\n","f389: 3.9727\n","f395: 1.0564\n","f396: 2.7307\n","f397: 3.3588\n","f398: 7.7882\n","f399: 10.5109\n","f400: 36.9772\n","f401: 17.6783\n","f402: 15.5905\n","f403: 11.0703\n","f404: 62.5719\n","f405: 110.9349\n","f406: 71.7388\n","f407: 123.0399\n","f408: 10.4633\n","f409: 15.4874\n","f410: 94.0787\n","f411: 5.9839\n","f412: 3.1602\n","f413: 4.7883\n","f414: 5.1289\n","f415: 6.4570\n","f416: 5.2767\n","f417: 4.4799\n","f418: 1.4879\n","f423: 2.0719\n","f424: 1.6355\n","f425: 5.9597\n","f426: 6.3903\n","f427: 12.8335\n","f428: 18.7241\n","f429: 36.3825\n","f430: 15.6529\n","f431: 19.6650\n","f432: 36.5810\n","f433: 26.7623\n","f434: 20.6771\n","f435: 147.2637\n","f436: 9.0083\n","f437: 125.1990\n","f438: 46.6285\n","f439: 8.0594\n","f440: 5.1236\n","f441: 7.9425\n","f442: 13.1925\n","f443: 5.7459\n","f444: 4.2763\n","f445: 5.7849\n","f446: 1.8059\n","f451: 2.6026\n","f452: 1.4489\n","f453: 14.6649\n","f454: 4.7521\n","f455: 18.9040\n","f456: 8.7327\n","f457: 35.4027\n","f458: 15.1882\n","f459: 12.6050\n","f460: 8.7732\n","f461: 9.7066\n","f462: 19.9887\n","f463: 15.9501\n","f464: 14.1196\n","f465: 10.8575\n","f466: 17.2114\n","f467: 8.3483\n","f468: 5.9346\n","f469: 8.4656\n","f470: 12.2120\n","f471: 9.0441\n","f472: 5.3632\n","f473: 36.0197\n","f474: 1.7033\n","f479: 4.6733\n","f480: 3.9017\n","f481: 5.9535\n","f482: 4.6096\n","f483: 6.3002\n","f484: 18.0122\n","f485: 21.4965\n","f486: 33.3220\n","f487: 51.9548\n","f488: 19.7939\n","f489: 65.1810\n","f490: 50.2430\n","f491: 6.7721\n","f492: 4.5382\n","f493: 9.9436\n","f494: 5.7185\n","f495: 4.6573\n","f496: 4.9234\n","f497: 7.9983\n","f498: 8.1101\n","f499: 15.1367\n","f500: 8.6696\n","f501: 4.3647\n","f506: 1.9242\n","f507: 11.6340\n","f508: 6.2592\n","f509: 5.0282\n","f510: 22.6916\n","f511: 28.5296\n","f512: 26.7632\n","f513: 10.0689\n","f514: 88.8582\n","f515: 21.6935\n","f516: 17.3617\n","f517: 37.6855\n","f518: 8.8690\n","f519: 7.9083\n","f520: 5.2080\n","f521: 8.9157\n","f522: 36.8677\n","f523: 9.8282\n","f524: 6.7852\n","f525: 8.6063\n","f526: 14.7703\n","f527: 48.9225\n","f528: 22.0278\n","f529: 4.8828\n","f535: 5.7137\n","f536: 7.0407\n","f537: 13.1702\n","f538: 18.0334\n","f539: 18.4231\n","f540: 29.5470\n","f541: 39.6404\n","f542: 50.9440\n","f543: 48.7140\n","f544: 12.1671\n","f545: 9.4475\n","f546: 5.2195\n","f547: 4.8077\n","f548: 7.6499\n","f549: 8.8825\n","f550: 49.7577\n","f551: 11.3646\n","f552: 5.1716\n","f553: 15.7455\n","f554: 7.4476\n","f555: 21.9853\n","f556: 21.1628\n","f557: 3.2399\n","f563: 26.6627\n","f564: 34.9070\n","f565: 5.6239\n","f566: 9.4191\n","f567: 26.6158\n","f568: 23.4806\n","f569: 88.8403\n","f570: 23.6642\n","f571: 8.6754\n","f572: 15.6180\n","f573: 8.9212\n","f574: 6.0636\n","f575: 7.0243\n","f576: 4.9948\n","f577: 6.8833\n","f578: 29.1139\n","f579: 7.8608\n","f580: 4.6046\n","f581: 15.0796\n","f582: 83.2260\n","f583: 154.6680\n","f584: 2.8871\n","f585: 1.5923\n","f586: 3.0602\n","f591: 1.3455\n","f592: 4.4798\n","f593: 1.4073\n","f594: 9.0211\n","f595: 10.1792\n","f596: 39.9981\n","f597: 48.1849\n","f598: 32.2573\n","f599: 6.6069\n","f600: 6.7892\n","f601: 4.9094\n","f602: 7.1002\n","f603: 6.0919\n","f604: 5.2738\n","f605: 6.6541\n","f606: 6.7160\n","f607: 7.5744\n","f608: 9.9585\n","f609: 3.0018\n","f610: 8.0598\n","f611: 7.3808\n","f612: 1.4803\n","f613: 0.8718\n","f619: 1.2501\n","f620: 2.1621\n","f621: 2.9913\n","f622: 13.8460\n","f623: 17.1584\n","f624: 13.6854\n","f625: 11.0508\n","f626: 10.0293\n","f627: 9.7123\n","f628: 10.5156\n","f629: 7.2655\n","f630: 4.9641\n","f631: 4.7292\n","f632: 4.8173\n","f633: 6.0537\n","f634: 6.2701\n","f635: 5.6732\n","f636: 7.8701\n","f637: 6.5383\n","f638: 18.5901\n","f639: 2.5945\n","f640: 1.6168\n","f641: 0.8094\n","f647: 1.6072\n","f648: 5.5978\n","f649: 2.4132\n","f650: 3.2921\n","f651: 3.8600\n","f652: 18.9814\n","f653: 14.2272\n","f654: 11.4434\n","f655: 20.7923\n","f656: 65.2674\n","f657: 116.1350\n","f658: 11.8548\n","f659: 13.6104\n","f660: 17.8554\n","f661: 9.7560\n","f662: 6.5318\n","f663: 5.7254\n","f664: 2.3749\n","f665: 2.8169\n","f666: 2.6234\n","f667: 5.4133\n","f668: 3.1763\n","f675: 2.6317\n","f676: 3.4738\n","f677: 3.8820\n","f678: 3.2783\n","f679: 3.3181\n","f680: 3.5551\n","f681: 10.9617\n","f682: 5.8222\n","f683: 7.1301\n","f684: 13.2093\n","f685: 9.4067\n","f686: 7.4651\n","f687: 14.0103\n","f688: 11.0036\n","f689: 4.9498\n","f690: 4.7232\n","f691: 3.5791\n","f692: 2.1009\n","f693: 3.7661\n","f694: 3.7775\n","f695: 7.5162\n","f696: 2.3402\n","f704: 1.2306\n","f705: 15.9986\n","f706: 17.2972\n","f707: 12.7266\n","f708: 78.0309\n","f709: 54.4384\n","f710: 6.3877\n","f711: 115.3135\n","f712: 8.7907\n","f713: 106.6715\n","f714: 48.9590\n","f715: 16.1612\n","f716: 64.9272\n","f717: 47.5153\n","f718: 6.8131\n","f719: 65.8348\n","f720: 47.6570\n","f721: 4.1445\n","f722: 31.2068\n","f732: 0.8319\n","f733: 1.1214\n","f734: 0.7946\n","f735: 1.1603\n","f736: 2.7738\n","f737: 1.7816\n","f738: 15.6559\n","f739: 2.9578\n","f740: 177.8944\n","f741: 13.6482\n","f742: 108.5351\n","f743: 78.4583\n","f744: 12.7619\n","f745: 65.9772\n","f746: 4.1137\n","f747: 3.5373\n","f748: 3.0128\n","f749: 6.3943\n","f750: 0.9557\n","\n","Feature Importances (Booster by Weight):\n","f43: 1.0\n","f44: 4.0\n","f45: 3.0\n","f66: 5.0\n","f67: 2.0\n","f68: 12.0\n","f69: 28.0\n","f70: 7.0\n","f71: 9.0\n","f72: 8.0\n","f73: 10.0\n","f74: 3.0\n","f75: 2.0\n","f76: 1.0\n","f91: 5.0\n","f92: 18.0\n","f93: 37.0\n","f94: 21.0\n","f95: 24.0\n","f96: 45.0\n","f97: 56.0\n","f98: 58.0\n","f99: 44.0\n","f100: 42.0\n","f101: 48.0\n","f102: 40.0\n","f103: 60.0\n","f104: 41.0\n","f105: 14.0\n","f106: 13.0\n","f107: 16.0\n","f108: 2.0\n","f117: 5.0\n","f118: 1.0\n","f119: 12.0\n","f120: 20.0\n","f121: 48.0\n","f122: 33.0\n","f123: 63.0\n","f124: 77.0\n","f125: 115.0\n","f126: 80.0\n","f127: 104.0\n","f128: 119.0\n","f129: 84.0\n","f130: 73.0\n","f131: 75.0\n","f132: 61.0\n","f133: 52.0\n","f134: 38.0\n","f135: 28.0\n","f136: 17.0\n","f137: 7.0\n","f143: 1.0\n","f144: 3.0\n","f145: 21.0\n","f146: 37.0\n","f147: 30.0\n","f148: 54.0\n","f149: 56.0\n","f150: 54.0\n","f151: 65.0\n","f152: 97.0\n","f153: 129.0\n","f154: 135.0\n","f155: 147.0\n","f156: 120.0\n","f157: 90.0\n","f158: 106.0\n","f159: 73.0\n","f160: 49.0\n","f161: 46.0\n","f162: 51.0\n","f163: 48.0\n","f164: 26.0\n","f165: 15.0\n","f166: 1.0\n","f171: 1.0\n","f172: 2.0\n","f173: 21.0\n","f174: 26.0\n","f175: 44.0\n","f176: 56.0\n","f177: 56.0\n","f178: 63.0\n","f179: 130.0\n","f180: 128.0\n","f181: 114.0\n","f182: 151.0\n","f183: 162.0\n","f184: 160.0\n","f185: 163.0\n","f186: 114.0\n","f187: 76.0\n","f188: 73.0\n","f189: 70.0\n","f190: 50.0\n","f191: 70.0\n","f192: 37.0\n","f193: 12.0\n","f199: 2.0\n","f200: 10.0\n","f201: 58.0\n","f202: 35.0\n","f203: 51.0\n","f204: 66.0\n","f205: 93.0\n","f206: 106.0\n","f207: 111.0\n","f208: 146.0\n","f209: 142.0\n","f210: 168.0\n","f211: 146.0\n","f212: 180.0\n","f213: 194.0\n","f214: 172.0\n","f215: 116.0\n","f216: 77.0\n","f217: 72.0\n","f218: 67.0\n","f219: 61.0\n","f220: 50.0\n","f221: 11.0\n","f222: 4.0\n","f227: 3.0\n","f228: 23.0\n","f229: 53.0\n","f230: 47.0\n","f231: 41.0\n","f232: 52.0\n","f233: 83.0\n","f234: 97.0\n","f235: 120.0\n","f236: 138.0\n","f237: 117.0\n","f238: 174.0\n","f239: 146.0\n","f240: 190.0\n","f241: 174.0\n","f242: 126.0\n","f243: 134.0\n","f244: 92.0\n","f245: 92.0\n","f246: 47.0\n","f247: 68.0\n","f248: 71.0\n","f249: 15.0\n","f250: 1.0\n","f255: 8.0\n","f256: 35.0\n","f257: 36.0\n","f258: 32.0\n","f259: 52.0\n","f260: 77.0\n","f261: 109.0\n","f262: 107.0\n","f263: 146.0\n","f264: 140.0\n","f265: 154.0\n","f266: 216.0\n","f267: 228.0\n","f268: 222.0\n","f269: 181.0\n","f270: 146.0\n","f271: 186.0\n","f272: 147.0\n","f273: 78.0\n","f274: 70.0\n","f275: 84.0\n","f276: 82.0\n","f277: 18.0\n","f278: 1.0\n","f282: 1.0\n","f283: 6.0\n","f284: 7.0\n","f285: 26.0\n","f286: 62.0\n","f287: 59.0\n","f288: 87.0\n","f289: 120.0\n","f290: 166.0\n","f291: 140.0\n","f292: 192.0\n","f293: 223.0\n","f294: 263.0\n","f295: 263.0\n","f296: 243.0\n","f297: 185.0\n","f298: 160.0\n","f299: 142.0\n","f300: 150.0\n","f301: 89.0\n","f302: 90.0\n","f303: 65.0\n","f304: 72.0\n","f305: 20.0\n","f306: 2.0\n","f311: 1.0\n","f312: 33.0\n","f313: 27.0\n","f314: 57.0\n","f315: 106.0\n","f316: 142.0\n","f317: 163.0\n","f318: 170.0\n","f319: 181.0\n","f320: 196.0\n","f321: 263.0\n","f322: 281.0\n","f323: 214.0\n","f324: 201.0\n","f325: 201.0\n","f326: 174.0\n","f327: 146.0\n","f328: 134.0\n","f329: 97.0\n","f330: 121.0\n","f331: 78.0\n","f332: 52.0\n","f333: 20.0\n","f334: 1.0\n","f339: 6.0\n","f340: 19.0\n","f341: 60.0\n","f342: 73.0\n","f343: 91.0\n","f344: 136.0\n","f345: 207.0\n","f346: 213.0\n","f347: 194.0\n","f348: 255.0\n","f349: 222.0\n","f350: 276.0\n","f351: 230.0\n","f352: 195.0\n","f353: 197.0\n","f354: 167.0\n","f355: 174.0\n","f356: 124.0\n","f357: 136.0\n","f358: 136.0\n","f359: 87.0\n","f360: 35.0\n","f361: 10.0\n","f362: 2.0\n","f367: 4.0\n","f368: 27.0\n","f369: 50.0\n","f370: 87.0\n","f371: 127.0\n","f372: 143.0\n","f373: 152.0\n","f374: 223.0\n","f375: 209.0\n","f376: 209.0\n","f377: 232.0\n","f378: 234.0\n","f379: 197.0\n","f380: 182.0\n","f381: 165.0\n","f382: 124.0\n","f383: 131.0\n","f384: 94.0\n","f385: 79.0\n","f386: 104.0\n","f387: 93.0\n","f388: 41.0\n","f389: 9.0\n","f395: 4.0\n","f396: 21.0\n","f397: 43.0\n","f398: 89.0\n","f399: 155.0\n","f400: 198.0\n","f401: 193.0\n","f402: 198.0\n","f403: 181.0\n","f404: 154.0\n","f405: 202.0\n","f406: 205.0\n","f407: 181.0\n","f408: 183.0\n","f409: 202.0\n","f410: 151.0\n","f411: 132.0\n","f412: 95.0\n","f413: 70.0\n","f414: 63.0\n","f415: 65.0\n","f416: 38.0\n","f417: 22.0\n","f418: 3.0\n","f423: 2.0\n","f424: 19.0\n","f425: 63.0\n","f426: 92.0\n","f427: 164.0\n","f428: 174.0\n","f429: 235.0\n","f430: 190.0\n","f431: 178.0\n","f432: 176.0\n","f433: 188.0\n","f434: 268.0\n","f435: 168.0\n","f436: 151.0\n","f437: 149.0\n","f438: 174.0\n","f439: 154.0\n","f440: 108.0\n","f441: 68.0\n","f442: 75.0\n","f443: 37.0\n","f444: 49.0\n","f445: 13.0\n","f446: 2.0\n","f451: 6.0\n","f452: 16.0\n","f453: 42.0\n","f454: 87.0\n","f455: 148.0\n","f456: 183.0\n","f457: 215.0\n","f458: 177.0\n","f459: 199.0\n","f460: 219.0\n","f461: 207.0\n","f462: 223.0\n","f463: 138.0\n","f464: 152.0\n","f465: 116.0\n","f466: 122.0\n","f467: 126.0\n","f468: 83.0\n","f469: 60.0\n","f470: 71.0\n","f471: 46.0\n","f472: 26.0\n","f473: 25.0\n","f474: 2.0\n","f479: 6.0\n","f480: 15.0\n","f481: 43.0\n","f482: 52.0\n","f483: 112.0\n","f484: 142.0\n","f485: 192.0\n","f486: 201.0\n","f487: 219.0\n","f488: 214.0\n","f489: 237.0\n","f490: 186.0\n","f491: 159.0\n","f492: 137.0\n","f493: 115.0\n","f494: 98.0\n","f495: 78.0\n","f496: 57.0\n","f497: 58.0\n","f498: 30.0\n","f499: 35.0\n","f500: 24.0\n","f501: 22.0\n","f506: 1.0\n","f507: 14.0\n","f508: 17.0\n","f509: 42.0\n","f510: 59.0\n","f511: 125.0\n","f512: 172.0\n","f513: 139.0\n","f514: 189.0\n","f515: 200.0\n","f516: 265.0\n","f517: 211.0\n","f518: 165.0\n","f519: 123.0\n","f520: 131.0\n","f521: 128.0\n","f522: 77.0\n","f523: 76.0\n","f524: 44.0\n","f525: 56.0\n","f526: 27.0\n","f527: 23.0\n","f528: 47.0\n","f529: 16.0\n","f535: 8.0\n","f536: 29.0\n","f537: 33.0\n","f538: 54.0\n","f539: 94.0\n","f540: 131.0\n","f541: 142.0\n","f542: 194.0\n","f543: 180.0\n","f544: 198.0\n","f545: 134.0\n","f546: 123.0\n","f547: 119.0\n","f548: 108.0\n","f549: 80.0\n","f550: 104.0\n","f551: 88.0\n","f552: 68.0\n","f553: 61.0\n","f554: 47.0\n","f555: 36.0\n","f556: 32.0\n","f557: 7.0\n","f563: 17.0\n","f564: 26.0\n","f565: 29.0\n","f566: 49.0\n","f567: 88.0\n","f568: 114.0\n","f569: 154.0\n","f570: 143.0\n","f571: 152.0\n","f572: 149.0\n","f573: 142.0\n","f574: 127.0\n","f575: 135.0\n","f576: 82.0\n","f577: 92.0\n","f578: 74.0\n","f579: 73.0\n","f580: 57.0\n","f581: 69.0\n","f582: 59.0\n","f583: 21.0\n","f584: 13.0\n","f585: 2.0\n","f586: 1.0\n","f591: 6.0\n","f592: 25.0\n","f593: 22.0\n","f594: 70.0\n","f595: 78.0\n","f596: 89.0\n","f597: 122.0\n","f598: 119.0\n","f599: 108.0\n","f600: 109.0\n","f601: 109.0\n","f602: 98.0\n","f603: 91.0\n","f604: 74.0\n","f605: 92.0\n","f606: 90.0\n","f607: 78.0\n","f608: 57.0\n","f609: 45.0\n","f610: 39.0\n","f611: 33.0\n","f612: 13.0\n","f613: 1.0\n","f619: 3.0\n","f620: 50.0\n","f621: 32.0\n","f622: 58.0\n","f623: 58.0\n","f624: 68.0\n","f625: 104.0\n","f626: 111.0\n","f627: 113.0\n","f628: 103.0\n","f629: 123.0\n","f630: 116.0\n","f631: 97.0\n","f632: 97.0\n","f633: 69.0\n","f634: 65.0\n","f635: 85.0\n","f636: 50.0\n","f637: 43.0\n","f638: 28.0\n","f639: 8.0\n","f640: 9.0\n","f641: 1.0\n","f647: 4.0\n","f648: 25.0\n","f649: 43.0\n","f650: 53.0\n","f651: 57.0\n","f652: 53.0\n","f653: 97.0\n","f654: 107.0\n","f655: 93.0\n","f656: 141.0\n","f657: 113.0\n","f658: 123.0\n","f659: 123.0\n","f660: 88.0\n","f661: 83.0\n","f662: 107.0\n","f663: 56.0\n","f664: 49.0\n","f665: 37.0\n","f666: 10.0\n","f667: 19.0\n","f668: 9.0\n","f675: 1.0\n","f676: 16.0\n","f677: 54.0\n","f678: 69.0\n","f679: 67.0\n","f680: 59.0\n","f681: 90.0\n","f682: 122.0\n","f683: 85.0\n","f684: 78.0\n","f685: 107.0\n","f686: 97.0\n","f687: 75.0\n","f688: 65.0\n","f689: 75.0\n","f690: 49.0\n","f691: 61.0\n","f692: 35.0\n","f693: 19.0\n","f694: 20.0\n","f695: 5.0\n","f696: 3.0\n","f704: 2.0\n","f705: 13.0\n","f706: 26.0\n","f707: 45.0\n","f708: 35.0\n","f709: 64.0\n","f710: 51.0\n","f711: 58.0\n","f712: 64.0\n","f713: 43.0\n","f714: 47.0\n","f715: 45.0\n","f716: 32.0\n","f717: 42.0\n","f718: 31.0\n","f719: 26.0\n","f720: 14.0\n","f721: 7.0\n","f722: 7.0\n","f732: 1.0\n","f733: 8.0\n","f734: 7.0\n","f735: 1.0\n","f736: 3.0\n","f737: 4.0\n","f738: 16.0\n","f739: 9.0\n","f740: 26.0\n","f741: 15.0\n","f742: 21.0\n","f743: 34.0\n","f744: 7.0\n","f745: 19.0\n","f746: 5.0\n","f747: 3.0\n","f748: 2.0\n","f749: 3.0\n","f750: 1.0\n"]}]},{"cell_type":"markdown","source":["\n","# LightGBM\n","\n","LightGBM is a high-performance gradient boosting framework optimized for **speed and memory efficiency**, particularly on **large datasets**. It introduces three major improvements over traditional boosting methods:\n","\n","1. **Leaf-Wise Tree Growth**: Prioritizes splits that yield the highest loss reduction.\n","2. **Gradient-Based One-Side Sampling (GOSS)**: Efficiently reduces training data size while preserving crucial gradient information.\n","3. **Exclusive Feature Bundling (EFB)**: Compresses sparse high-dimensional data by bundling mutually exclusive features.\n","\n","\n","\n","## Leaf-Wise Tree Growth\n","\n","Instead of growing trees **depth-wise** (splitting all leaves at a given depth before moving to the next), LightGBM grows trees **leaf-wise (best-first)**:\n","\n","- At each step, it **splits the leaf** that results in the highest loss reduction.\n","- This allows LightGBM to **focus on the most informative regions** of the feature space, leading to faster error minimization.\n","- **Potential drawback**: This can lead to **very deep trees** in localized regions, increasing the risk of overfitting. Regularization via `max_depth` and `num_leaves` is important.\n","\n","\n","\n","##  Gradient-Based One-Side Sampling (GOSS)\n","\n","###  High Training Cost on Large Data\n","\n","Gradient boosting requires computing **gradients and Hessians for all data points** in every iteration to determine the best tree split. When datasets are **very large**, this becomes computationally expensive. A naive solution is **random subsampling**, but this risks discarding important information.\n","\n","**GOSS** selectively **keeps** the most **informative** data points:\n","\n","1. **Large-gradient examples** (hard-to-predict points) are **fully retained** because they contribute most to model updates.\n","2. **Small-gradient examples** (already well-predicted points) are **partially discarded** via random subsampling to save computation.\n","3. **Rescaling** ensures the overall gradient statistics remain correct.\n","\n","By keeping the strongest gradient signals while reducing redundancy, LightGBM **reduces computational cost while maintaining accuracy**.\n","\n","\n","\n","### The GOSS Algorithm\n","\n","1. **Compute Gradients**:  \n","   Compute the first-order gradient $g_i$ for each training instance $i$.\n","\n","2. **Sort by Gradient Magnitude**:  \n","   Rank all data points by $|g_i|$ in descending order.\n","\n","3. **Select Two Sets**:  \n","   - $A$: The top $a$ fraction of data points with the **largest** gradients (fully retained).\n","   - $B'$: The bottom $(1-a)$ fraction of small-gradient points (potentially sampled).\n","\n","4. **Sample from the Small-Gradient Set**:  \n","   From $B'$, randomly select a fraction $b$. The final training set becomes:\n","   $$\n","   S = A \\cup B\n","   $$\n","   - $|A| = aN$ (all kept).\n","   - $|B| = b(1-a)N$ (randomly sampled).\n","\n","5. **Re-Scale Small Gradients**:  \n","   To correct for sampling bias, each gradient in $B$ is **upweighted**:\n","   $$\n","   g'_i = \\alpha g_i, \\quad \\text{where } \\alpha = \\frac{1-a}{b}.\n","   $$\n","   This ensures the total gradient sum remains unbiased.\n","\n","6. **Train the Tree on the Reduced Set**:  \n","   LightGBM constructs the tree using only $S$, significantly reducing computation.\n","\n","\n","## Exclusive Feature Bundling (EFB)\n","\n","\n","Many real-world datasets have **thousands or millions of features**, particularly in:\n","\n","- **One-hot encodings** of categorical variables.\n","- **Sparse matrices** (e.g., text data, recommender systems).\n","\n","Processing these features **individually** is expensive in terms of **memory** and **computation**.\n","\n","\n","**EFB** identifies **mutually exclusive** features—features that are **never nonzero at the same time**—and **combines them into a single feature**.\n","\n","For example:\n","\n","| Feature 1 | Feature 2 | Feature 3 |\n","|-----------|-----------|-----------|\n","| 1         | 0         | 0         |\n","| 0         | 1         | 0         |\n","| 0         | 0         | 1         |\n","\n","Since at most **one feature is nonzero per row**, these can be **merged into a single feature index**, reducing memory usage and computation.\n","\n","\n","\n","###  How EFB Works\n","\n","1. **Identify Groups of Exclusive Features**:\n","   - Construct a **graph** where each feature is a node.\n","   - Connect features if they **overlap too much** (i.e., are nonzero in the same rows).\n","   - Solve this as a **graph coloring** problem to minimize the number of feature groups.\n","\n","2. **Bundle Features into a Shared Histogram**:\n","   - Each group of features is **combined into a single histogram**.\n","   - Within the histogram, each feature **uses a separate bin range**.\n","   - This drastically **reduces the number of histograms** needed.\n"],"metadata":{"id":"D96eAi7Cx_TF"}},{"cell_type":"code","source":["import lightgbm as lgb\n","\n","clf = lgb.LGBMClassifier(\n","    objective='multiclass',\n","    num_class=10,\n","    learning_rate=0.1,\n","    n_estimators=100,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42\n",")\n","\n","# Train the classifier with early stopping.\n","# The eval_set allows the model to monitor performance on the test set and stop early if no improvement is seen.\n","clf.fit(\n","    X_train, y_train,\n","    eval_set=[(X_test, y_test)],\n","    eval_metric='multi_logloss', callbacks=[lgb.log_evaluation(period=1)]\n",")\n","\n","# Make predictions on the test set and evaluate accuracy.\n","y_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"LGBMClassifier Accuracy on MNIST: {:.2f}%\".format(accuracy * 100))\n","\n"],"metadata":{"id":"bMxdDqrKyBJ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738942427406,"user_tz":300,"elapsed":161152,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"4e9f83aa-69eb-4254-8547-43642324d828"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.105216 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 109563\n","[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 632\n","[LightGBM] [Info] Start training from score -2.316515\n","[LightGBM] [Info] Start training from score -2.184506\n","[LightGBM] [Info] Start training from score -2.304086\n","[LightGBM] [Info] Start training from score -2.282619\n","[LightGBM] [Info] Start training from score -2.328074\n","[LightGBM] [Info] Start training from score -2.405911\n","[LightGBM] [Info] Start training from score -2.320410\n","[LightGBM] [Info] Start training from score -2.261603\n","[LightGBM] [Info] Start training from score -2.327903\n","[LightGBM] [Info] Start training from score -2.308603\n","[1]\tvalid_0's multi_logloss: 1.69342\n","[2]\tvalid_0's multi_logloss: 1.39071\n","[3]\tvalid_0's multi_logloss: 1.17623\n","[4]\tvalid_0's multi_logloss: 1.01611\n","[5]\tvalid_0's multi_logloss: 0.889818\n","[6]\tvalid_0's multi_logloss: 0.788155\n","[7]\tvalid_0's multi_logloss: 0.702904\n","[8]\tvalid_0's multi_logloss: 0.630863\n","[9]\tvalid_0's multi_logloss: 0.57012\n","[10]\tvalid_0's multi_logloss: 0.519245\n","[11]\tvalid_0's multi_logloss: 0.475545\n","[12]\tvalid_0's multi_logloss: 0.438017\n","[13]\tvalid_0's multi_logloss: 0.405358\n","[14]\tvalid_0's multi_logloss: 0.377099\n","[15]\tvalid_0's multi_logloss: 0.350643\n","[16]\tvalid_0's multi_logloss: 0.327642\n","[17]\tvalid_0's multi_logloss: 0.307784\n","[18]\tvalid_0's multi_logloss: 0.289424\n","[19]\tvalid_0's multi_logloss: 0.273856\n","[20]\tvalid_0's multi_logloss: 0.259176\n","[21]\tvalid_0's multi_logloss: 0.246381\n","[22]\tvalid_0's multi_logloss: 0.234679\n","[23]\tvalid_0's multi_logloss: 0.224314\n","[24]\tvalid_0's multi_logloss: 0.214832\n","[25]\tvalid_0's multi_logloss: 0.206223\n","[26]\tvalid_0's multi_logloss: 0.197706\n","[27]\tvalid_0's multi_logloss: 0.190741\n","[28]\tvalid_0's multi_logloss: 0.184099\n","[29]\tvalid_0's multi_logloss: 0.178566\n","[30]\tvalid_0's multi_logloss: 0.172978\n","[31]\tvalid_0's multi_logloss: 0.167398\n","[32]\tvalid_0's multi_logloss: 0.16269\n","[33]\tvalid_0's multi_logloss: 0.158274\n","[34]\tvalid_0's multi_logloss: 0.153658\n","[35]\tvalid_0's multi_logloss: 0.149617\n","[36]\tvalid_0's multi_logloss: 0.145933\n","[37]\tvalid_0's multi_logloss: 0.142442\n","[38]\tvalid_0's multi_logloss: 0.139161\n","[39]\tvalid_0's multi_logloss: 0.136257\n","[40]\tvalid_0's multi_logloss: 0.13311\n","[41]\tvalid_0's multi_logloss: 0.130613\n","[42]\tvalid_0's multi_logloss: 0.128227\n","[43]\tvalid_0's multi_logloss: 0.125847\n","[44]\tvalid_0's multi_logloss: 0.123617\n","[45]\tvalid_0's multi_logloss: 0.121708\n","[46]\tvalid_0's multi_logloss: 0.119705\n","[47]\tvalid_0's multi_logloss: 0.117749\n","[48]\tvalid_0's multi_logloss: 0.115856\n","[49]\tvalid_0's multi_logloss: 0.11432\n","[50]\tvalid_0's multi_logloss: 0.112538\n","[51]\tvalid_0's multi_logloss: 0.110723\n","[52]\tvalid_0's multi_logloss: 0.109181\n","[53]\tvalid_0's multi_logloss: 0.107782\n","[54]\tvalid_0's multi_logloss: 0.10617\n","[55]\tvalid_0's multi_logloss: 0.104572\n","[56]\tvalid_0's multi_logloss: 0.103284\n","[57]\tvalid_0's multi_logloss: 0.102233\n","[58]\tvalid_0's multi_logloss: 0.100874\n","[59]\tvalid_0's multi_logloss: 0.099664\n","[60]\tvalid_0's multi_logloss: 0.0984843\n","[61]\tvalid_0's multi_logloss: 0.0975601\n","[62]\tvalid_0's multi_logloss: 0.0965734\n","[63]\tvalid_0's multi_logloss: 0.0954575\n","[64]\tvalid_0's multi_logloss: 0.0947181\n","[65]\tvalid_0's multi_logloss: 0.093664\n","[66]\tvalid_0's multi_logloss: 0.092846\n","[67]\tvalid_0's multi_logloss: 0.0921629\n","[68]\tvalid_0's multi_logloss: 0.0911584\n","[69]\tvalid_0's multi_logloss: 0.0903426\n","[70]\tvalid_0's multi_logloss: 0.0897133\n","[71]\tvalid_0's multi_logloss: 0.0891572\n","[72]\tvalid_0's multi_logloss: 0.0882945\n","[73]\tvalid_0's multi_logloss: 0.0877773\n","[74]\tvalid_0's multi_logloss: 0.0871566\n","[75]\tvalid_0's multi_logloss: 0.086669\n","[76]\tvalid_0's multi_logloss: 0.0861147\n","[77]\tvalid_0's multi_logloss: 0.0855842\n","[78]\tvalid_0's multi_logloss: 0.0851468\n","[79]\tvalid_0's multi_logloss: 0.0846742\n","[80]\tvalid_0's multi_logloss: 0.0840745\n","[81]\tvalid_0's multi_logloss: 0.0837333\n","[82]\tvalid_0's multi_logloss: 0.0832895\n","[83]\tvalid_0's multi_logloss: 0.0827521\n","[84]\tvalid_0's multi_logloss: 0.0823838\n","[85]\tvalid_0's multi_logloss: 0.0817448\n","[86]\tvalid_0's multi_logloss: 0.0813676\n","[87]\tvalid_0's multi_logloss: 0.0808083\n","[88]\tvalid_0's multi_logloss: 0.0804309\n","[89]\tvalid_0's multi_logloss: 0.0800129\n","[90]\tvalid_0's multi_logloss: 0.0796566\n","[91]\tvalid_0's multi_logloss: 0.0794271\n","[92]\tvalid_0's multi_logloss: 0.0790747\n","[93]\tvalid_0's multi_logloss: 0.0787878\n","[94]\tvalid_0's multi_logloss: 0.0785193\n","[95]\tvalid_0's multi_logloss: 0.0782567\n","[96]\tvalid_0's multi_logloss: 0.0780346\n","[97]\tvalid_0's multi_logloss: 0.0778505\n","[98]\tvalid_0's multi_logloss: 0.0774792\n","[99]\tvalid_0's multi_logloss: 0.0770987\n","[100]\tvalid_0's multi_logloss: 0.0769959\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["LGBMClassifier Accuracy on MNIST: 97.59%\n"]}]},{"cell_type":"markdown","source":["# HistGradientBoosting\n","\n","The **HistGradientBoostingRegressor** is a histogram-based gradient boosting method that improves efficiency by binning continuous features and using histograms to optimize split selection. Its **three core innovations** are:\n","\n","1. **Histogram-Based Binning**: Converts continuous feature values into discrete bins, reducing computation time.\n","2. **Efficient Split Finding**: Uses precomputed histograms to quickly determine optimal splits.\n","3. **Regularization via Binning**: The discretization process reduces noise and improves generalization.\n","\n","\n","## Binning the Features\n","\n","Each feature $j$ is **mapped to a bin index** before training:\n","$$\n","b_{ij} = \\mathcal{B}_j(x_{ij}) \\in \\{0, 1, \\dots, B-1\\}.\n","$$\n","where:\n","- $ x_{ij} $ is the raw feature value.\n","- $ \\mathcal{B}_j $ is a **binning function** (e.g., uniform binning, quantile-based binning).\n","- $B$ is the **number of bins per feature** (typically much smaller than the number of unique values).\n","\n","### Advantages of Binning\n","\n","- **Computational Speed-Up**: The number of candidate splits is reduced from **unique values** to **only $B$ bins**.\n","- **Memory Efficiency**: Instead of storing raw feature values, we store **integer bin indices**, reducing memory usage.\n","- **Regularization Effect**: Binning smooths out **noise** in feature values, reducing overfitting.\n","\n"],"metadata":{"id":"1SoBlPtrem52"}},{"cell_type":"code","source":["\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","\n","clf = HistGradientBoostingClassifier(\n","    max_iter=100,        # Total number of boosting rounds\n","    learning_rate=0.1,   # Step size shrinkage\n","    max_depth=10,        # Maximum tree depth for base learners\n","    random_state=42,\n","    verbose=1\n",")\n","\n","# Train the classifier\n","clf.fit(X_train, y_train)\n","\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"HistGradientBoostingClassifier Accuracy on MNIST: {accuracy * 100:.2f}%\")"],"metadata":{"id":"u6_Pn4kIeohr","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1738942818979,"user_tz":300,"elapsed":236574,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"314f6473-063d-4978-d06c-ce48fac56767"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Binning 0.339 GB of training data: 11.084 s\n","Binning 0.038 GB of validation data: 0.249 s\n","Fitting gradient boosted rounds:\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-b77bcab5fb13>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# Build `n_trees_per_iteration` trees.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trees_per_iteration_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 grower = TreeGrower(\n\u001b[0m\u001b[1;32m    879\u001b[0m                     \u001b[0mX_binned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_binned_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                     \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_binned, gradients, hessians, max_leaf_nodes, max_depth, min_samples_leaf, min_gain_to_split, min_hessian_to_split, n_bins, n_bins_non_missing, has_missing_values, is_categorical, monotonic_cst, interaction_cst, l2_regularization, feature_fraction_per_split, rng, shrinkage, n_threads)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_apply_split_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m  \u001b[0;31m# time spent splitting nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_categorical_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessians\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\u001b[0m in \u001b[0;36m_initialize_root\u001b[0;34m(self, gradients, hessians)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         self.root.histograms = self.histogram_builder.compute_histograms_brute(\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# CatBoost\n","\n","CatBoost is a gradient boosting framework optimized for **handling categorical data** efficiently and **reducing overfitting**. It introduces three main innovations:\n","\n","1. **Oblivious (Symmetric) Decision Trees**: A highly regularized tree structure that ensures efficient computation.\n","2. **Ordered Target Statistics**: A principled method for encoding categorical features while preventing data leakage.\n","3. **Permutation-Driven Boosting**: A robust training procedure that stabilizes gradient estimates.\n","\n","\n","\n","##  Oblivious (Symmetric) Decision Trees\n","\n","\n","A tree $T(x)$ of depth $d$ is defined by:\n","- A set of features $\\{ f_1, f_2, \\dots, f_d \\}$ used for splitting.\n","- A set of thresholds $\\{ t_1, t_2, \\dots, t_d \\}$.\n","- Leaf values $\\{ l_0, l_1, \\dots, l_{2^d - 1} \\}$.\n","\n","For an input $x$, the decision at level $j$ is:\n","$$\n","b_j(x) = \\mathbb{I} \\{ x_{f_j} \\leq t_j \\}.\n","$$\n","The leaf index is computed as:\n","$$\n","\\ell(x) = \\sum_{j=1}^{d} 2^{j-1} \\, b_j(x).\n","$$\n","The final tree prediction is:\n","$$\n","T(x) = l_{\\ell(x)}.\n","$$\n","This **regularizes the model**, reducing variance and **speeding up inference**, as tree traversal is **bitwise-computable**.\n","\n","\n","\n","##  Ordered Target Statistics for Categorical Features\n","\n","###  Handling Categorical Features\n","\n","Most boosting frameworks convert categorical variables using **one-hot encoding** or **mean target encoding**. However:\n","- **One-hot encoding** creates **high-dimensional sparse features**.\n","- **Mean target encoding** suffers from **target leakage** if computed over the full dataset.\n","\n","CatBoost **avoids leakage** by computing categorical transformations **in an online manner** using **ordered statistics**.\n","\n","\n","\n","### Ordered Target Encoding\n","\n","For a categorical feature $C$, each category $c_i$ is transformed into:\n","$$\n","\\phi(c_i) = \\frac{\\sum_{j < i} \\mathbb{I}\\{ c_j = c_i \\} y_j + a \\gamma}{\\sum_{j < i} \\mathbb{I}\\{ c_j = c_i \\} + a}.\n","$$\n","where:\n","- **Summation is only over past samples** ($j < i$), ensuring **no information leakage**.\n","- **$a$** is a regularization parameter (prior weight).\n","- **$\\gamma$** is a prior value (e.g., global mean of $y$).\n","\n","This ensures that each sample **does not** see its own label during encoding.\n","\n","\n","\n","### Permutation-Driven Encoding\n","\n","Instead of **one fixed ordering**, CatBoost applies **multiple random permutations** of the dataset, computing:\n","$$\n","\\phi_{\\text{final}}(c_i) = \\frac{1}{P} \\sum_{p=1}^{P} \\phi^{(p)}(c_i),\n","$$\n","where $\\phi^{(p)}(c_i)$ is computed using permutation $p$.\n","\n","**Benefits:**\n","- **Reduces variance** in categorical transformations.\n","- **Improves generalization** across different training orderings.\n","\n","\n"],"metadata":{"id":"ZSiWqXoCdHlT"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","#########################################\n","# 1. Load and Preprocess the Adult Data #\n","#########################################\n","\n","# Fetch the Adult dataset from OpenML as a DataFrame.\n","adult = fetch_openml(\"adult\", version=2, as_frame=True)\n","df = adult.frame\n","\n","# Replace missing values represented by \"?\" with NaN, then drop rows with missing values.\n","df.replace(\"?\", np.nan, inplace=True)\n","df.dropna(inplace=True)\n","\n","# The target column is named \"class\" (income level)\n","target = \"class\"\n","\n","# Separate features and target.\n","X = df.drop(columns=[target])\n","y = df[target]\n","# Identify categorical features: here, we assume that columns with object dtype are categorical.\n","cat_features = X.select_dtypes(include=['category']).columns.tolist()\n","\n","# Perform a stratified train-test split to preserve class proportions.\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# For CatBoost, determine the indices of the categorical features (based on the training DataFrame).\n","cat_feature_indices = [X_train.columns.get_loc(col) for col in cat_features]\n","\n"],"metadata":{"id":"U2HZmCOnfnL9","executionInfo":{"status":"ok","timestamp":1738943230952,"user_tz":300,"elapsed":354,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["cat_features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlgrF-lbEggv","executionInfo":{"status":"ok","timestamp":1738943238578,"user_tz":300,"elapsed":19,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"07240abc-c91c-459a-bd3b-67d9d9aa45f7"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['workclass',\n"," 'education',\n"," 'marital-status',\n"," 'occupation',\n"," 'relationship',\n"," 'race',\n"," 'sex',\n"," 'native-country']"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsZLXR8fEmP3","executionInfo":{"status":"ok","timestamp":1738942893963,"user_tz":300,"elapsed":18182,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"87edf31e-a272-4de0-c223-c0f319be472b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.8)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}]},{"cell_type":"code","source":["#########################################\n","# 2. Train and Evaluate CatBoost Model  #\n","#########################################\n","\n","from catboost import CatBoostClassifier\n","\n","# Initialize CatBoostClassifier.\n","# - iterations: Number of boosting rounds.\n","# - learning_rate: Shrinks the contribution of each tree.\n","# - depth: Maximum tree depth.\n","# - verbose: Prints progress messages.\n","cat_model = CatBoostClassifier(\n","    iterations=500,\n","    learning_rate=0.1,\n","    depth=6,\n","    random_seed=42,\n","    verbose=50\n",")\n","\n","# Train the model, passing the categorical feature indices.\n","cat_model.fit(X_train, y_train, cat_features=cat_feature_indices)\n","\n","# Predict on the test set.\n","y_pred_cat = cat_model.predict(X_test)\n","\n","# Evaluate CatBoost's performance.\n","acc_cat = accuracy_score(y_test, y_pred_cat)\n","print(\"CatBoost Accuracy:\", acc_cat)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7Ks1jI4EdBe","executionInfo":{"status":"ok","timestamp":1738943382146,"user_tz":300,"elapsed":38431,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"2cb1ad65-c0e7-4e24-ad97-a3bc8bb334a0"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 0.6020203\ttotal: 63.3ms\tremaining: 31.6s\n","50:\tlearn: 0.2928853\ttotal: 4.61s\tremaining: 40.6s\n","100:\tlearn: 0.2817301\ttotal: 8.39s\tremaining: 33.1s\n","150:\tlearn: 0.2741182\ttotal: 11.6s\tremaining: 26.8s\n","200:\tlearn: 0.2691135\ttotal: 14.8s\tremaining: 22.1s\n","250:\tlearn: 0.2646418\ttotal: 19.9s\tremaining: 19.7s\n","300:\tlearn: 0.2607285\ttotal: 23.1s\tremaining: 15.3s\n","350:\tlearn: 0.2574644\ttotal: 26.4s\tremaining: 11.2s\n","400:\tlearn: 0.2535783\ttotal: 29.7s\tremaining: 7.34s\n","450:\tlearn: 0.2509368\ttotal: 34.8s\tremaining: 3.78s\n","499:\tlearn: 0.2480219\ttotal: 38.1s\tremaining: 0us\n","CatBoost Accuracy: 0.867551133222775\n"]}]},{"cell_type":"code","source":["\n","#########################################\n","# 3. Train and Evaluate LightGBM Model  #\n","#########################################\n","\n","import lightgbm as lgb\n","from lightgbm import LGBMClassifier\n","\n","# Initialize the LightGBM classifier.\n","lgb_model = LGBMClassifier(\n","    n_estimators=500,\n","    learning_rate=0.1,\n","    max_depth=6,\n","    random_state=42\n",")\n","\n","# Train the model.\n","# For LightGBM, pass the categorical feature names.\n","lgb_model.fit(X_train, y_train, categorical_feature=cat_features,callbacks=[lgb.log_evaluation(period=1)])\n","\n","# Predict on the test set.\n","y_pred_lgb = lgb_model.predict(X_test)\n","\n","# Evaluate LightGBM's performance.\n","acc_lgb = accuracy_score(y_test, y_pred_lgb)\n","print(\"LightGBM Accuracy:\", acc_lgb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOG1MYX6GUGa","executionInfo":{"status":"ok","timestamp":1738943384727,"user_tz":300,"elapsed":2579,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"0953ebd9-c78f-4e75-b32d-6b92742b1941"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 8966, number of negative: 27211\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004453 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 708\n","[LightGBM] [Info] Number of data points in the train set: 36177, number of used features: 14\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.247837 -> initscore=-1.110182\n","[LightGBM] [Info] Start training from score -1.110182\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","LightGBM Accuracy: 0.8652294085129906\n"]}]}]}